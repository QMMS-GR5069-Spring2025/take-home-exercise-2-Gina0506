{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ea3c3f-079a-499e-8964-0a5dd119741f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gina Wang\n",
    "# Dr. Morales & Nana\n",
    "# QMSSGR5069 Applied Data Sciences - Take Home Exercise #2\n",
    "# March 24, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6529b248-b304-4a1e-b088-612d1bece4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, round, concat_ws, trim, when, substring, upper, to_date, year, month, dayofmonth, min as spark_min, max as spark_max, count, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d85843b-9b68-4764-a630-6bc7fa57661c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: What was the average time each driver spent at the pit stop for each race?\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"F1 Pit Stop Analysis\").getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "pit_stops_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Compute average pit stop time (in milliseconds) for each driver in each race\n",
    "avg_pit_df = pit_stops_df.groupBy(\"raceId\", \"driverId\") \\\n",
    "    .agg(round(avg(\"milliseconds\"), 2).alias(\"avg_pitstop_time_ms\"))\n",
    "\n",
    "# Step 2: Prepare driver names\n",
    "drivers_df = drivers_df.select(\"driverId\", \"forename\", \"surname\")\n",
    "\n",
    "# Step 3: Join average pit stop times with driver names\n",
    "avg_pit_with_names = avg_pit_df.join(drivers_df, on=\"driverId\", how=\"left\") \\\n",
    "    .withColumn(\"driver_name\", concat_ws(\" \", trim(col(\"forename\")), trim(col(\"surname\")))) \\\n",
    "    .select(\"raceId\", \"driver_name\", \"avg_pitstop_time_ms\") \\\n",
    "    .orderBy(\"raceId\", \"avg_pitstop_time_ms\")\n",
    "\n",
    "# Step 4: Show results\n",
    "avg_pit_with_names.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab5a294-e2df-497a-b666-001f340f4f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: Rank the average time spent at the pit stop in order of who won each race\n",
    "\n",
    "from pyspark.sql.functions import avg, round, concat_ws, col, trim\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Load the data\n",
    "results_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "races_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Average pit stop time per driver per race\n",
    "avg_pit_df = (\n",
    "    pit_stops_df.groupBy(\"raceId\", \"driverId\")\n",
    "    .agg(round(avg(\"milliseconds\"), 2).alias(\"avg_pitstop_time_ms\"))\n",
    ")\n",
    "\n",
    "# Step 2: Join with results to get finishing positions\n",
    "joined_df = (\n",
    "    avg_pit_df.join(\n",
    "        results_df.select(\"raceId\", \"driverId\", \"positionOrder\"),\n",
    "        on=[\"raceId\", \"driverId\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Join with drivers to get full names\n",
    "joined_df = (\n",
    "    joined_df.join(\n",
    "        drivers_df.select(\"driverId\", \"forename\", \"surname\"),\n",
    "        on=\"driverId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\"driver_name\", concat_ws(\" \", trim(col(\"forename\")), trim(col(\"surname\"))))\n",
    ")\n",
    "\n",
    "# Step 4: Join with race name\n",
    "joined_df = (\n",
    "    joined_df.join(\n",
    "        races_df.select(\"raceId\", \"name\").withColumnRenamed(\"name\", \"race_name\"),\n",
    "        on=\"raceId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 5: Window to rank drivers by position (1 = winner)\n",
    "window_spec = Window.partitionBy(\"raceId\").orderBy(col(\"positionOrder\").asc_nulls_last())\n",
    "\n",
    "# Step 6: Add finishing rank column\n",
    "ranked_df = joined_df.withColumn(\"finishing_rank\", rank().over(window_spec))\n",
    "\n",
    "# Step 7: Final selection and sort\n",
    "final_df = (\n",
    "    ranked_df.select(\"raceId\", \"race_name\", \"driverId\", \"driver_name\",\n",
    "                     \"avg_pitstop_time_ms\", \"positionOrder\", \"finishing_rank\")\n",
    "    .orderBy(\"raceId\", \"finishing_rank\")\n",
    ")\n",
    "\n",
    "# Step 8: Show results\n",
    "final_df.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410003ba-a977-4fbc-9502-5dc03609e364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 3: Insert the missing code (e.g: ALO for Alonso) for drivers based on the 'drivers' dataset\n",
    "\n",
    "# Load the drivers dataset\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Replace missing codes: when code is null or equals '\\N', generate code from surname\n",
    "drivers_df_filled = drivers_df.withColumn(\n",
    "    \"code\",\n",
    "    when(\n",
    "        (col(\"code\").isNull()) | (col(\"code\") == \"\\\\N\"),\n",
    "        upper(substring(col(\"surname\"), 1, 3))\n",
    "    ).otherwise(col(\"code\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "drivers_df_filled.select(\"driverId\", \"forename\", \"surname\", \"code\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f366851-4803-4d9f-8d40-43264289b021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 4: Who is the youngest and oldest driver for each race? Create a new column called “Age”?\n",
    "\n",
    "# Load datasets\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "results_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "races_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Convert string columns to date type\n",
    "drivers_df = drivers_df.withColumn(\"dob\", to_date(col(\"dob\")))\n",
    "races_df = races_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "# Step 2: Join results with drivers and race info\n",
    "joined_df = (\n",
    "    results_df.join(drivers_df, on=\"driverId\", how=\"left\")\n",
    "              .join(races_df.select(\"raceId\", \"date\", \"name\"), on=\"raceId\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Step 3: Calculate age of driver on race day\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Age\",\n",
    "    when(\n",
    "        (month(col(\"date\")) > month(col(\"dob\"))) |\n",
    "        ((month(col(\"date\")) == month(col(\"dob\"))) & (dayofmonth(col(\"date\")) >= dayofmonth(col(\"dob\")))),\n",
    "        year(col(\"date\")) - year(col(\"dob\"))\n",
    "    ).otherwise(\n",
    "        year(col(\"date\")) - year(col(\"dob\")) - 1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 4: Define window to get youngest and oldest per race\n",
    "window_spec = Window.partitionBy(\"raceId\")\n",
    "\n",
    "# Step 5: Add youngest and oldest ages\n",
    "result_df = (\n",
    "    joined_df.withColumn(\"min_age\", spark_min(\"Age\").over(window_spec))\n",
    "             .withColumn(\"max_age\", spark_max(\"Age\").over(window_spec))\n",
    ")\n",
    "\n",
    "# Step 6: Filter for youngest or oldest drivers only\n",
    "extremes_df = result_df.filter(\n",
    "    (col(\"Age\") == col(\"min_age\")) | (col(\"Age\") == col(\"max_age\"))\n",
    ")\n",
    "\n",
    "# Step 7: Label as \"Youngest\" or \"Oldest\"\n",
    "extremes_df = extremes_df.withColumn(\n",
    "    \"Age_Category\",\n",
    "    when(col(\"Age\") == col(\"min_age\"), \"Youngest\").otherwise(\"Oldest\")\n",
    ")\n",
    "\n",
    "# Step 8: Final selection\n",
    "final_df = (\n",
    "    extremes_df.select(\n",
    "        \"raceId\", \"name\", \"date\", \"driverId\", \"forename\", \"surname\", \"Age\", \"Age_Category\"\n",
    "    ).orderBy(\"raceId\", \"Age_Category\")\n",
    ")\n",
    "\n",
    "# Step 9: Show results\n",
    "final_df.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071fb7a8-c574-4f52-b5c1-128fafd0dddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 5: For a given race, which driver has the most wins and losses?\n",
    "\n",
    "# Load data\n",
    "results_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "status_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/status.csv\", header=True, inferSchema=True)\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Get statusIds for DNF (not \"Finished\")\n",
    "dnf_status_ids = (\n",
    "    status_df.filter(col(\"status\") != \"Finished\")\n",
    "             .select(\"statusId\")\n",
    "             .rdd.flatMap(lambda x: x)\n",
    "             .collect()\n",
    ")\n",
    "\n",
    "# Step 2: Choose a specific raceId (you can change this)\n",
    "target_race_id = 841\n",
    "\n",
    "# Step 3: Filter for races before that race\n",
    "previous_races_df = results_df.filter(col(\"raceId\") < target_race_id)\n",
    "\n",
    "# Step 4: Add labeled columns for win, DNF, completed-not-won, and total participation\n",
    "labeled_df = (\n",
    "    previous_races_df.withColumn(\"win\", when(col(\"positionOrder\") == 1, 1).otherwise(0))\n",
    "                     .withColumn(\"not_completed\", when(col(\"statusId\").isin(dnf_status_ids), 1).otherwise(0))\n",
    "                     .withColumn(\n",
    "                         \"completed_not_won\",\n",
    "                         when((col(\"positionOrder\") > 1) & (~col(\"statusId\").isin(dnf_status_ids)), 1).otherwise(0)\n",
    "                     )\n",
    "                     .withColumn(\"total_participated\", lit(1))\n",
    ")\n",
    "\n",
    "# Step 5: Aggregate stats by driverId\n",
    "summary_df = (\n",
    "    labeled_df.groupBy(\"driverId\")\n",
    "              .agg(\n",
    "                  count(when(col(\"win\") == 1, True)).alias(\"wins\"),\n",
    "                  count(when(col(\"completed_not_won\") == 1, True)).alias(\"completed_not_won\"),\n",
    "                  count(when(col(\"not_completed\") == 1, True)).alias(\"not_completed\"),\n",
    "                  count(col(\"total_participated\")).alias(\"total_races\")\n",
    "              )\n",
    ")\n",
    "\n",
    "# Step 6: Join with driver names\n",
    "final_df = (\n",
    "    summary_df.join(drivers_df.select(\"driverId\", \"forename\", \"surname\"), on=\"driverId\", how=\"left\")\n",
    "              .select(\"driverId\", \"forename\", \"surname\", \"wins\", \"completed_not_won\", \"not_completed\", \"total_races\")\n",
    "              .orderBy(col(\"wins\").desc())\n",
    ")\n",
    "\n",
    "# Step 7: Show result\n",
    "final_df.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ccde7e-33d3-4867-95e0-52a07a048d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 6: Continue exploring the data by answering your own question.\n",
    "\n",
    "# My question is \"Which driver had the fastest average pit stop time in the 2021 season?\"\n",
    "\n",
    "# Load datasets\n",
    "races_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True)\n",
    "pit_stops_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Filter races for 2021 season\n",
    "races_2021 = (\n",
    "    races_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "            .filter(year(col(\"date\")) == 2021)\n",
    "            .select(\"raceId\", \"name\", \"date\")\n",
    ")\n",
    "\n",
    "# Step 2: Join races with pit stops to get only 2021 pit stops\n",
    "pit_stops_2021 = pit_stops_df.join(races_2021, on=\"raceId\", how=\"inner\")\n",
    "\n",
    "# Step 3: Calculate average pit stop time per driver\n",
    "avg_pitstop_by_driver = (\n",
    "    pit_stops_2021.groupBy(\"driverId\")\n",
    "                  .agg(round(avg(\"milliseconds\"), 2).alias(\"avg_pitstop_time_ms\"))\n",
    ")\n",
    "\n",
    "# Step 4: Add driver names\n",
    "final_df = (\n",
    "    avg_pitstop_by_driver.join(drivers_df.select(\"driverId\", \"forename\", \"surname\"), on=\"driverId\", how=\"left\")\n",
    "                         .withColumn(\"driver_name\", concat_ws(\" \", trim(col(\"forename\")), trim(col(\"surname\"))))\n",
    "                         .select(\"driverId\", \"driver_name\", \"avg_pitstop_time_ms\")\n",
    "                         .orderBy(\"avg_pitstop_time_ms\")\n",
    ")\n",
    "\n",
    "# Step 5: Show result\n",
    "final_df.show(50, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Take_Home_Exercise_#2_GinaWang",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
